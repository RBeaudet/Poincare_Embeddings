{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the article on Poincarré Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\robin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Packages import \n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from math import *\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('wordnet')  \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But : \n",
    "- Partir d'un mot wordnet.synset(\"mot\")\n",
    "- Construire un dictionnaire représentant le graphe lié à ce mot, du type {\"mot\" : [liste des hyponymes]}\n",
    "- Construire un dictionnaire du type {\"mot\" : niveau du mot}\n",
    "- Construire un dictionnaire des mots embedded du type {\"mot\" : vecteur}\n",
    "\n",
    "Dans le dictionnaire représentant le graphe, un même mot peut être clé ou valeur.\n",
    "Dans le dictionnaire des mots embedded, chaque mot est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any warm-blooded vertebrate having the skin more or less covered with hair; young are born alive except for the small subclass of monotremes and nourished with milk\n",
      "-------------------------\n",
      "Synset('mammal.n.01')\n"
     ]
    }
   ],
   "source": [
    "# Choose a source word for our graph, here the word \"mammal\", whose level in the graph is 0 (by default)\n",
    "\n",
    "mammal = wordnet.synset(\"mammal.n.01\")\n",
    "print(mammal.definition())  # definition of \"mammal\"\n",
    "print('-------------------------')\n",
    "print(mammal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('female_mammal.n.01'),\n",
       " Synset('fossorial_mammal.n.01'),\n",
       " Synset('metatherian.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('prototherian.n.01'),\n",
       " Synset('tusker.n.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyponyms of the source word, i.e. its direct children in the graph\n",
    "mammal.hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Training of the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Poincarre_Embeddings:\n",
    "    \n",
    "    def __init__(self, epochs, learning_rate, nb_negs, root_node, dimension):\n",
    "        \"\"\"\n",
    "        Object providing the embedding for words related by hypermnemy relations using \n",
    "        hyperbolic geometry.\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "        epochs -- number of epochs/iterations\n",
    "        learning_rate -- the learning rate for update of the embedding\n",
    "        nb_negs -- number of negative samples\n",
    "        root_node -- the higher word in the hierarchy, the format must be: wordnet.synset(\"word.n.01\")\n",
    "        dimension -- the embedding dimension\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nb_negs = nb_negs\n",
    "        self.root_node = root_node\n",
    "        self.dimension = dimension\n",
    "        \n",
    "    # Sample graph as a dictionnary\n",
    "\n",
    "    def sample_graph(self, root_node, max_level = 2) :\n",
    "        \"\"\"\n",
    "        Function that samples a hierarchical network from a root node and its hyponyms.\n",
    "        :param root_node: root node of the network\n",
    "        :param max_level: (int) maximum level of the network\n",
    "        :return graph: dictionnary representing the graph {\"node\" : [hyponyms]}\n",
    "        :return levels: dictionnary representing the level of each node {\"node\" : level}\n",
    "        \"\"\"\n",
    "\n",
    "        graph = {}\n",
    "\n",
    "        # keep track of visited nodes\n",
    "        explored = []\n",
    "\n",
    "        # keep track of nodes to be checked\n",
    "        queue = [root_node]\n",
    "\n",
    "        levels = {}\n",
    "        levels[str(root_node)] = 0\n",
    "\n",
    "        visited = [str(root_node)]\n",
    "\n",
    "        while queue:\n",
    "\n",
    "            # take out first node from queue\n",
    "            node = queue.pop(0)  # node n'est PAS un str\n",
    "\n",
    "            # condition on maximum level\n",
    "            if levels[str(node)] == max_level:\n",
    "                graph[str(node)] = []\n",
    "                break;\n",
    "\n",
    "            # mark node as explored node\n",
    "            explored.append(str(node))  # explored est un str\n",
    "\n",
    "            # sample neighbours of node (i.e. its hyponyms)\n",
    "            neighbours = [neighbour for neighbour in node.hyponyms()]  # ce sont pas des str\n",
    "            neighbours_str = [str(neighbour) for neighbour in node.hyponyms()]\n",
    "\n",
    "            # add neighbours to the graph (as children of the node)\n",
    "            graph[str(node)] = neighbours_str\n",
    "\n",
    "            # add neighbours of node to queue\n",
    "            for neighbour in neighbours :   # no str\n",
    "                if str(neighbour) not in visited :\n",
    "                    queue.append(neighbour) # no str\n",
    "                    visited.append(str(neighbour))\n",
    "                    levels[str(neighbour)] = levels[str(node)] + 1\n",
    "\n",
    "        return graph, levels\n",
    "    \n",
    "    def sample_embeddings(self, graph, dimension):\n",
    "        \"\"\"\n",
    "        Initializes embedded vectors of graph.\n",
    "        :param graph: graph containing words\n",
    "        :param dimension: (int) dimension of the embedding space\n",
    "        :return embeddings: dictionnary of the form {\"node\" : vector}\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings = {}\n",
    "\n",
    "        for word in graph:\n",
    "            embeddings[word] = np.random.uniform(low=-0.001, high=0.001, size=(dimension,))\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    def create_embeddings(self, dimension, root_node):\n",
    "        \"\"\"\n",
    "        Creates embeddings for words\n",
    "        \"\"\"\n",
    "        graph, levels = self.sample_graph(self.root_node) \n",
    "        embeddings = self.sample_embeddings(graph, self.dimension)\n",
    "        \n",
    "        return graph, embeddings\n",
    "        \n",
    "\n",
    "    def dist(self, u, v):\n",
    "        \"\"\"\n",
    "        Computes the distance for the Poincaré disk model between two vectors u and v\n",
    "\n",
    "        Arguments:\n",
    "        u -- first embedded object\n",
    "        v -- second embedded object\n",
    "\n",
    "        Returns:\n",
    "        z -- the ditance between the two objects\n",
    "        \"\"\"\n",
    "\n",
    "        assert norm(u) < 1 and norm(v) < 1\n",
    "        norm2u = norm(u)**2\n",
    "        norm2v = norm(u)**2\n",
    "        norm2distuv = norm(u - v)**2\n",
    "        t = 1 + 2 * (norm2distuv / ((1 - norm2u) * (1 - norm2v)))\n",
    "        z = np.arccosh(t)\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def pdr(self, theta, x):\n",
    "        \"\"\"\n",
    "        Computes the partial derivative w.r.t theta\n",
    "\n",
    "        Arguments:\n",
    "        theta -- embedding of the object\n",
    "        x -- vector corresponding to the embedding of another object (same dimension as theta)\n",
    "\n",
    "        Returns:\n",
    "        partial -- the derivative (same dimension as theta)  \n",
    "        \"\"\"\n",
    "\n",
    "        alpha = 1.0 - norm(theta)**2\n",
    "        assert len(alpha.shape) == 0\n",
    "        beta = 1.0 - norm(x)**2\n",
    "        assert len(beta.shape) == 0\n",
    "        gamma = 1 + (2 / (alpha * beta)) * norm(alpha - x)**2\n",
    "        assert len(gamma.shape) == 0\n",
    "        partial = 4.0 / (beta * np.sqrt(gamma**2 - 1)) * (((norm(x) - 2 * np.dot(theta, x) + 1) / alpha**2) * theta - (x / alpha))\n",
    "\n",
    "        return partial\n",
    "\n",
    "    def proj(self, theta, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        Projection in the Poincaré disk ball\n",
    "\n",
    "        Parameters:\n",
    "        theta --  embedding of the object\n",
    "        epsilon -- scalar (for stability)\n",
    "\n",
    "        Returns:\n",
    "        theta -- after projection\n",
    "        \"\"\"\n",
    "\n",
    "        if norm(theta) >= 1:\n",
    "            theta = (theta / norm(theta)) - epsilon\n",
    "\n",
    "        return theta\n",
    "\n",
    "    def update(self, theta, grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Computes the full update for a single embedding of theta\n",
    "\n",
    "        Parameters:\n",
    "        theta -- current embedding of the object\n",
    "        grad -- gradient of the loss function \n",
    "        learning_rate -- the learning rate \n",
    "\n",
    "        Returns:\n",
    "        theta -- the updated theta\n",
    "        \"\"\"\n",
    "\n",
    "        upd = (learning_rate / 4) * (1 - norm(theta)**2)**2 * grad\n",
    "        theta = self.proj(theta - upd)\n",
    "        assert theta.shape == upd.shape\n",
    "\n",
    "        return theta\n",
    "    \n",
    "    def loss(self, u, v, negative_samples):\n",
    "        \"\"\"\n",
    "        Computes the loss for a single couple of related nodes (u,v)\n",
    "\n",
    "        Arguments:\n",
    "        u -- embedding of one object\n",
    "        v -- embedding of one object\n",
    "        negative_samples -- set of negative samples for u including v\n",
    "\n",
    "        Returns:\n",
    "        loss -- the value of the loss\n",
    "        \"\"\"\n",
    "        negative_distances = [np.exp(-self.dist(u, k)) for k in negative_samples]\n",
    "        loss = -self.dist(u, v) - np.log(np.sum(negative_distances))\n",
    "\n",
    "        return loss \n",
    "    \n",
    "    def pdl(self, u, negative_samples):\n",
    "        \"\"\"\n",
    "        Computes the partial derivative of the loss w.r.t d(u,v), d(u,v'), where v' is a negative example for u\n",
    "        \n",
    "        Arguments:\n",
    "        u -- embedding of one object\n",
    "        negative_samples -- list of negative samples for u\n",
    "        positive -- boolean, computes the partial derivative of the loss w.r.t d(u,v) if True\n",
    "        \n",
    "        Returns:\n",
    "        derivative -- the partial derivative (scalar or list)\n",
    "        \"\"\"\n",
    "    \n",
    "        negative_distances = [np.exp(-self.dist(u, k)) for k in negative_samples]\n",
    "        derivative = [np.exp(self.dist(u, k)) / np.sum(negative_distances) for k in negative_samples]\n",
    "\n",
    "        return derivative \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        graph, embeddings = self.create_embeddings(self.dimension, self.root_node)\n",
    "        embeddings_temp = embeddings.copy()\n",
    "        \n",
    "        ## Select couple (u, v) and negative samples for u\n",
    "        for u in embeddings_temp:\n",
    "            if len(graph[u]) == 0:\n",
    "                continue\n",
    "            \n",
    "            for v in graph[u]:\n",
    "                if v not in graph.keys():\n",
    "                    graph[v] = []\n",
    "                    embeddings[v] = np.random.uniform(low=-0.001, high=0.001, size=(self.dimension,))\n",
    "                \n",
    "            # Select v among the hyponyms of u\n",
    "            #v = np.random.choice(graph[u])\n",
    "            #if v not in graph.keys():\n",
    "                #graph[v] = []\n",
    "                #embeddings[v] = np.random.uniform(low=-0.001, high=0.001, size=(self.dimension,))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # Select word\n",
    "            for u in embeddings:\n",
    "                if len(graph[u]) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Select v among the hyponyms of u\n",
    "                v = np.random.choice(graph[u])\n",
    "                #if v not in graph.keys():\n",
    "                    #graph[v] = []\n",
    "                    #embeddings[v] = np.random.uniform(low=-0.001, high=0.001, size=(self.dimension,))\n",
    "                \n",
    "                # Select negative examples for u\n",
    "                negative_samples, negative_samples_words = [], []  # list of vectors/list of words\n",
    "                \n",
    "                while len(negative_samples) < self.nb_negs:\n",
    "\n",
    "                    # draw sample randomly from data\n",
    "                    negative_sample = np.random.choice(list(embeddings.keys()))\n",
    "\n",
    "                    # if the drawn sample is connected to u, discard it\n",
    "                    if negative_sample in graph[u] or u in graph[negative_sample] or negative_sample == u:\n",
    "                        continue \n",
    "\n",
    "                    negative_samples_words.append(negative_sample)\n",
    "                    negative_sample = embeddings[negative_sample]\n",
    "                    negative_samples.append(negative_sample)\n",
    "\n",
    "\n",
    "                ## Compute the individual loss\n",
    "                loss = self.loss(embeddings[u], embeddings[v], negative_samples)\n",
    "\n",
    "                ## Compute the partial derivatives of the loss with respect to u, v and the negative examples\n",
    "\n",
    "                # derivative of loss with respect to u\n",
    "                grad_u = -1.0 * self.pdr(embeddings[u], embeddings[v])\n",
    "\n",
    "                # derivative of loss with respect to v\n",
    "                grad_v = -1.0 * self.pdr(embeddings[v], embeddings[u])\n",
    "\n",
    "                # derivative of loss with respect to the negative examples\n",
    "                grad_negatives = []\n",
    "                grad_negatives_temp = self.pdl(embeddings[u], negative_samples)\n",
    "\n",
    "                for (negative_sample, grad_negative) in zip(negative_samples, grad_negatives_temp):\n",
    "                    gradient = grad_negative * self.pdr(negative_sample, embeddings[u])\n",
    "                    grad_negatives.append(gradient)\n",
    "\n",
    "                ## Update embeddings\n",
    "\n",
    "                # update u\n",
    "                embeddings[u] = self.update(embeddings[u], grad_u, self.learning_rate)\n",
    "\n",
    "                # update v\n",
    "                embeddings[v] = self.update(embeddings[v], grad_v, self.learning_rate)\n",
    "\n",
    "                # update negative samples\n",
    "                for (negative_sample, grad_negative, negative_sample_word) in zip(negative_samples, grad_negatives, negative_samples_words):\n",
    "                        embeddings[negative_sample_word] = self.update(negative_sample, grad_negative, self.learning_rate)\n",
    "                    \n",
    "        plot_graph_2D(embeddings)\n",
    "                    \n",
    "         \n",
    "    @staticmethod\n",
    "    def plot_graph_2D(embeddings):\n",
    "        \"\"\"\n",
    "        Function that allows to plot the embedded vectors when the embedding space is 2 dimensional\n",
    "        \"\"\"\n",
    "\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        # plot all the nodes\n",
    "        #for word in embeddings:\n",
    "        #plt.plot(embeddings[word][0], embeddings[word][1], 'bo', label = word)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        for word in embeddings:\n",
    "            ax.scatter(embeddings[word][0], embeddings[word][1])\n",
    "            for i, word in enumerate(embeddings):\n",
    "                ax.annotate(word, (embeddings[word][0], embeddings[word][1]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22ba8a48198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAFkCAYAAADRxGotAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhhhJkpQlQ4wkScqSIUaSJGXp/wPaqdE36xE1YQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22ba8a23ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mammal = wordnet.synset(\"mammal.n.01\")\n",
    "cls = Poincarre_Embeddings(1, 0.01, 3, mammal, 2)\n",
    "cls.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mammal = wordnet.synset(\"mammal.n.01\")\n",
    "cls = Poincarre_Embeddings(10, 0.01, 3, mammal, 2)\n",
    "graph, embeddings = cls.create_embeddings(2, mammal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Synset('damaraland_mole_rat.n.01')\": [],\n",
       " \"Synset('female_mammal.n.01')\": [],\n",
       " \"Synset('fossorial_mammal.n.01')\": [\"Synset('damaraland_mole_rat.n.01')\",\n",
       "  \"Synset('naked_mole_rat.n.01')\"],\n",
       " \"Synset('mammal.n.01')\": [\"Synset('female_mammal.n.01')\",\n",
       "  \"Synset('fossorial_mammal.n.01')\",\n",
       "  \"Synset('metatherian.n.01')\",\n",
       "  \"Synset('placental.n.01')\",\n",
       "  \"Synset('prototherian.n.01')\",\n",
       "  \"Synset('tusker.n.01')\"],\n",
       " \"Synset('metatherian.n.01')\": [\"Synset('marsupial.n.01')\"],\n",
       " \"Synset('placental.n.01')\": [\"Synset('aardvark.n.01')\",\n",
       "  \"Synset('aquatic_mammal.n.01')\",\n",
       "  \"Synset('bat.n.01')\",\n",
       "  \"Synset('buck.n.05')\",\n",
       "  \"Synset('bull.n.11')\",\n",
       "  \"Synset('carnivore.n.01')\",\n",
       "  \"Synset('cow.n.02')\",\n",
       "  \"Synset('digitigrade_mammal.n.01')\",\n",
       "  \"Synset('doe.n.02')\",\n",
       "  \"Synset('edentate.n.01')\",\n",
       "  \"Synset('fissipedia.n.01')\",\n",
       "  \"Synset('flying_lemur.n.01')\",\n",
       "  \"Synset('hyrax.n.01')\",\n",
       "  \"Synset('insectivore.n.01')\",\n",
       "  \"Synset('lagomorph.n.01')\",\n",
       "  \"Synset('livestock.n.01')\",\n",
       "  \"Synset('pachyderm.n.01')\",\n",
       "  \"Synset('pangolin.n.01')\",\n",
       "  \"Synset('plantigrade_mammal.n.01')\",\n",
       "  \"Synset('primate.n.02')\",\n",
       "  \"Synset('proboscidean.n.01')\",\n",
       "  \"Synset('rodent.n.01')\",\n",
       "  \"Synset('tree_shrew.n.01')\",\n",
       "  \"Synset('unguiculata.n.01')\",\n",
       "  \"Synset('unguiculate.n.01')\",\n",
       "  \"Synset('ungulata.n.01')\",\n",
       "  \"Synset('ungulate.n.01')\",\n",
       "  \"Synset('yearling.n.03')\"],\n",
       " \"Synset('prototherian.n.01')\": [\"Synset('monotreme.n.01')\"],\n",
       " \"Synset('tusker.n.01')\": []}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([\"Synset('prototherian.n.01')\", \"Synset('placental.n.01')\", \"Synset('tusker.n.01')\", \"Synset('metatherian.n.01')\", \"Synset('female_mammal.n.01')\", \"Synset('mammal.n.01')\", \"Synset('damaraland_mole_rat.n.01')\", \"Synset('fossorial_mammal.n.01')\"])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u is:  Synset('prototherian.n.01')\n",
      "v is:  Synset('monotreme.n.01')\n",
      "negative_samples_words : [\"Synset('female_mammal.n.01')\", \"Synset('tusker.n.01')\", \"Synset('metatherian.n.01')\"]\n",
      "negative_samples : [array([-0.00064182,  0.00020491]), array([0.0001629 , 0.00058987]), array([ 0.00081248, -0.00060515])]\n",
      "-----------------------------------------------------\n",
      "u is:  Synset('placental.n.01')\n",
      "v is:  Synset('edentate.n.01')\n",
      "negative_samples_words : [\"Synset('female_mammal.n.01')\", \"Synset('damaraland_mole_rat.n.01')\", \"Synset('fossorial_mammal.n.01')\"]\n",
      "negative_samples : [array([-0.00064182,  0.00020491]), array([-0.00014274, -0.00013274]), array([-0.00064724,  0.00094159])]\n",
      "-----------------------------------------------------\n",
      "u is:  Synset('metatherian.n.01')\n",
      "v is:  Synset('marsupial.n.01')\n",
      "negative_samples_words : [\"Synset('placental.n.01')\", \"Synset('placental.n.01')\", \"Synset('female_mammal.n.01')\"]\n",
      "negative_samples : [array([ 0.00064961, -0.00026617]), array([ 0.00064961, -0.00026617]), array([-0.00064182,  0.00020491])]\n",
      "-----------------------------------------------------\n",
      "u is:  Synset('mammal.n.01')\n",
      "v is:  Synset('fossorial_mammal.n.01')\n",
      "negative_samples_words : [\"Synset('damaraland_mole_rat.n.01')\", \"Synset('damaraland_mole_rat.n.01')\", \"Synset('damaraland_mole_rat.n.01')\"]\n",
      "negative_samples : [array([-0.00014274, -0.00013274]), array([-0.00014274, -0.00013274]), array([-0.00014274, -0.00013274])]\n",
      "-----------------------------------------------------\n",
      "u is:  Synset('fossorial_mammal.n.01')\n",
      "v is:  Synset('naked_mole_rat.n.01')\n",
      "negative_samples_words : [\"Synset('placental.n.01')\", \"Synset('tusker.n.01')\", \"Synset('placental.n.01')\"]\n",
      "negative_samples : [array([ 0.00064961, -0.00026617]), array([0.0001629 , 0.00058987]), array([ 0.00064961, -0.00026617])]\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for u in embeddings:\n",
    "    if len(graph[u]) == 0:\n",
    "        continue\n",
    "    print(\"u is: \", u)\n",
    "                    \n",
    "    # Select v among the hyponyms of u\n",
    "    v = np.random.choice(graph[u])\n",
    "    print(\"v is: \", v)\n",
    "                \n",
    "    # Select negative examples for u\n",
    "    negative_samples, negative_samples_words = [], []  # list of vectors/list of words\n",
    "                \n",
    "    while len(negative_samples) < 3:\n",
    "\n",
    "        # draw sample randomly from data\n",
    "        negative_sample = np.random.choice(list(embeddings.keys()))\n",
    "\n",
    "        # if the drawn sample is connected to u, discard it\n",
    "        if negative_sample in graph[u] or u in graph[negative_sample] or negative_sample == u:\n",
    "            continue \n",
    "\n",
    "        negative_samples_words.append(negative_sample)\n",
    "        negative_sample = embeddings[negative_sample]\n",
    "        negative_samples.append(negative_sample)\n",
    "        \n",
    "    print(\"negative_samples_words : {0}\".format(negative_samples_words))\n",
    "    print(\"negative_samples : {0}\".format(negative_samples))\n",
    "    print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the article on Poincarré Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\robin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Packages import \n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from math import *\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('wordnet')  \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But : \n",
    "- Partir d'un mot wordnet.synset(\"mot\")\n",
    "- Construire un dictionnaire représentant le graphe lié à ce mot, du type {\"mot\" : [liste des hyponymes]}\n",
    "- Construire un dictionnaire du type {\"mot\" : niveau du mot}\n",
    "- Construire un dictionnaire des mots embedded du type {\"mot\" : vecteur}\n",
    "\n",
    "Dans le dictionnaire représentant le graphe, un même mot peut être clé ou valeur.\n",
    "Dans le dictionnaire des mots embedded, chaque mot est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any warm-blooded vertebrate having the skin more or less covered with hair; young are born alive except for the small subclass of monotremes and nourished with milk\n",
      "-------------------------\n",
      "Synset('mammal.n.01')\n"
     ]
    }
   ],
   "source": [
    "# Choose a source word for our graph, here the word \"mammal\", whose level in the graph is 0 (by default)\n",
    "\n",
    "mammal = wordnet.synset(\"mammal.n.01\")\n",
    "print(mammal.definition())  # definition of \"mammal\"\n",
    "print('-------------------------')\n",
    "print(mammal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('female_mammal.n.01'),\n",
       " Synset('fossorial_mammal.n.01'),\n",
       " Synset('metatherian.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('prototherian.n.01'),\n",
       " Synset('tusker.n.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyponyms of the source word, i.e. its direct children in the graph\n",
    "mammal.hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Training of the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Poincarre_Embeddings:\n",
    "    \n",
    "    def __init__(self, epochs, learning_rate, nb_negs, root_node, dimension):\n",
    "        \"\"\"\n",
    "        Object providing the embedding for words related by hypermnemy relations using \n",
    "        hyperbolic geometry.\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "        epochs -- number of epochs/iterations\n",
    "        learning_rate -- the learning rate for update of the embedding\n",
    "        nb_negs -- number of negative samples\n",
    "        root_node -- the higher word in the hierarchy, the format must be: wordnet.synset(\"word.n.01\")\n",
    "        dimension -- the embedding dimension\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nb_negs = nb_negs\n",
    "        self.root_node = root_node\n",
    "        self.dimension = dimension\n",
    "        \n",
    "    # Sample graph as a dictionnary\n",
    "\n",
    "    def sample_graph(self, root_node, max_level = 7) :\n",
    "        \"\"\"\n",
    "        Function that samples a hierarchical network from a root node and its hyponyms.\n",
    "        :param root_node: root node of the network\n",
    "        :param max_level: (int) maximum level of the network\n",
    "        :return graph: dictionnary representing the graph {\"node\" : [hyponyms]}\n",
    "        :return levels: dictionnary representing the level of each node {\"node\" : level}\n",
    "        \"\"\"\n",
    "\n",
    "        graph = {}\n",
    "\n",
    "        # keep track of visited nodes\n",
    "        explored = []\n",
    "\n",
    "        # keep track of nodes to be checked\n",
    "        queue = [root_node]\n",
    "\n",
    "        levels = {}\n",
    "        levels[str(root_node)] = 0\n",
    "\n",
    "        visited = [str(root_node)]\n",
    "\n",
    "        while queue:\n",
    "\n",
    "            # take out first node from queue\n",
    "            node = queue.pop(0)  # node n'est PAS un str\n",
    "\n",
    "            # condition on maximum level\n",
    "            if levels[str(node)] == max_level:\n",
    "                graph[str(node)] = []\n",
    "                break;\n",
    "\n",
    "            # mark node as explored node\n",
    "            explored.append(str(node))  # explored est un str\n",
    "\n",
    "            # sample neighbours of node (i.e. its hyponyms)\n",
    "            neighbours = [neighbour for neighbour in node.hyponyms()]  # ce sont pas des str\n",
    "            neighbours_str = [str(neighbour) for neighbour in node.hyponyms()]\n",
    "\n",
    "            # add neighbours to the graph (as children of the node)\n",
    "            graph[str(node)] = neighbours_str\n",
    "\n",
    "            # add neighbours of node to queue\n",
    "            for neighbour in neighbours :   # no str\n",
    "                if str(neighbour) not in visited :\n",
    "                    queue.append(neighbour) # no str\n",
    "                    visited.append(str(neighbour))\n",
    "                    levels[str(neighbour)] = levels[str(node)] + 1\n",
    "\n",
    "        return graph, levels\n",
    "    \n",
    "    def sample_embeddings(self, graph, dimension):\n",
    "        \"\"\"\n",
    "        Initializes embedded vectors of graph.\n",
    "        :param graph: graph containing words\n",
    "        :param dimension: (int) dimension of the embedding space\n",
    "        :return embeddings: dictionnary of the form {\"node\" : vector}\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings = {}\n",
    "\n",
    "        for word in graph:\n",
    "            embeddings[word] = np.random.uniform(low=-0.001, high=0.001, size=(dimension,))\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    def create_embeddings(self, dimension, root_node):\n",
    "        \"\"\"\n",
    "        Creates embeddings for words\n",
    "        \"\"\"\n",
    "        graph, levels = self.sample_graph(self.root_node) \n",
    "        embeddings = self.sample_embeddings(graph, self.dimension)\n",
    "        \n",
    "        return graph, embeddings\n",
    "        \n",
    "\n",
    "    def dist(self, u, v):\n",
    "        \"\"\"\n",
    "        Computes the distance for the Poincaré disk model between two vectors u and v\n",
    "\n",
    "        Arguments:\n",
    "        u -- first embedded object\n",
    "        v -- second embedded object\n",
    "\n",
    "        Returns:\n",
    "        z -- the ditance between the two objects\n",
    "        \"\"\"\n",
    "\n",
    "        assert norm(u) < 1 and norm(v) < 1\n",
    "        norm2u = norm(u)**2\n",
    "        norm2v = norm(u)**2\n",
    "        norm2distuv = norm(u - v)**2\n",
    "        t = 1 + 2 * (norm2distuv / ((1 - norm2u) * (1 - norm2v)))\n",
    "        z = np.arccosh(t)\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def pdr(self, theta, x):\n",
    "        \"\"\"\n",
    "        Computes the partial derivative w.r.t theta\n",
    "\n",
    "        Arguments:\n",
    "        theta -- embedding of the object\n",
    "        x -- vector corresponding to the embedding of another object (same dimension as theta)\n",
    "\n",
    "        Returns:\n",
    "        partial -- the derivative (same dimension as theta)  \n",
    "        \"\"\"\n",
    "\n",
    "        alpha = 1.0 - norm(theta)**2\n",
    "        assert len(alpha.shape) == 0\n",
    "        beta = 1.0 - norm(x)**2\n",
    "        assert len(beta.shape) == 0\n",
    "        gamma = 1 + (2 / (alpha * beta)) * norm(alpha - x)**2\n",
    "        assert len(gamma.shape) == 0\n",
    "        partial = 4.0 / (beta * np.sqrt(gamma**2 - 1)) * (((norm(x) - 2 * np.dot(theta, x) + 1) / alpha**2) * theta - (x / alpha))\n",
    "\n",
    "        return partial\n",
    "\n",
    "    def proj(self, theta, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        Projection in the Poincaré disk ball\n",
    "\n",
    "        Parameters:\n",
    "        theta --  embedding of the object\n",
    "        epsilon -- scalar (for stability)\n",
    "\n",
    "        Returns:\n",
    "        theta -- after projection\n",
    "        \"\"\"\n",
    "\n",
    "        if norm(theta) >= 1:\n",
    "            theta = (theta / norm(theta)) - epsilon\n",
    "\n",
    "        return theta\n",
    "\n",
    "    def update(self, theta, grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Computes the full update for a single embedding of theta\n",
    "\n",
    "        Parameters:\n",
    "        theta -- current embedding of the object\n",
    "        grad -- gradient of the loss function \n",
    "        learning_rate -- the learning rate \n",
    "\n",
    "        Returns:\n",
    "        theta -- the updated theta\n",
    "        \"\"\"\n",
    "\n",
    "        upd = (learning_rate / 4) * (1 - norm(theta)**2)**2 * grad\n",
    "        theta = self.proj(theta - upd)\n",
    "        assert theta.shape == upd.shape\n",
    "\n",
    "        return theta\n",
    "    \n",
    "    def loss(self, u, v, negative_samples):\n",
    "        \"\"\"\n",
    "        Computes the loss for a single couple of related nodes (u,v)\n",
    "\n",
    "        Arguments:\n",
    "        u -- embedding of one object\n",
    "        v -- embedding of one object\n",
    "        negative_samples -- set of negative samples for u including v\n",
    "\n",
    "        Returns:\n",
    "        loss -- the value of the loss\n",
    "        \"\"\"\n",
    "        negative_distances = [np.exp(-self.dist(u, k)) for k in negative_samples]\n",
    "        loss = -self.dist(u, v) - np.log(np.sum(negative_distances))\n",
    "\n",
    "        return loss \n",
    "    \n",
    "    def pdl(self, u, negative_samples):\n",
    "        \"\"\"\n",
    "        Computes the partial derivative of the loss w.r.t d(u,v), d(u,v'), where v' is a negative example for u\n",
    "        \n",
    "        Arguments:\n",
    "        u -- embedding of one object\n",
    "        negative_samples -- list of negative samples for u\n",
    "        positive -- boolean, computes the partial derivative of the loss w.r.t d(u,v) if True\n",
    "        \n",
    "        Returns:\n",
    "        derivative -- the partial derivative (scalar or list)\n",
    "        \"\"\"\n",
    "    \n",
    "        negative_distances = [np.exp(-self.dist(u, k)) for k in negative_samples]\n",
    "        derivative = [np.exp(self.dist(u, k)) / np.sum(negative_distances) for k in negative_samples]\n",
    "\n",
    "        return derivative \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        graph, embeddings = self.create_embeddings(self.dimension, self.root_node)\n",
    "        embeddings_temp = embeddings.copy()\n",
    "        \n",
    "        ## Select couple (u, v) and negative samples for u\n",
    "        for u in embeddings_temp:\n",
    "            if len(graph[u]) == 0:\n",
    "                continue\n",
    "            \n",
    "            for v in graph[u]:\n",
    "                if v not in graph.keys():\n",
    "                    graph[v] = []\n",
    "                    embeddings[v] = np.random.uniform(low=-0.001, high=0.001, size=(self.dimension,))\n",
    "                \n",
    "            # Select v among the hyponyms of u\n",
    "            #v = np.random.choice(graph[u])\n",
    "            #if v not in graph.keys():\n",
    "                #graph[v] = []\n",
    "                #embeddings[v] = np.random.uniform(low=-0.001, high=0.001, size=(self.dimension,))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 10 == 0:\n",
    "                print(epoch)\n",
    "\n",
    "            # Select word\n",
    "            for u in embeddings:\n",
    "                if len(graph[u]) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Select v among the hyponyms of u\n",
    "                v = np.random.choice(graph[u])\n",
    "                #if v not in graph.keys():\n",
    "                    #graph[v] = []\n",
    "                    #embeddings[v] = np.random.uniform(low=-0.001, high=0.001, size=(self.dimension,))\n",
    "                \n",
    "                # Select negative examples for u\n",
    "                negative_samples, negative_samples_words = [], []  # list of vectors/list of words\n",
    "                \n",
    "                while len(negative_samples) < self.nb_negs:\n",
    "\n",
    "                    # draw sample randomly from data\n",
    "                    negative_sample = np.random.choice(list(embeddings.keys()))\n",
    "\n",
    "                    # if the drawn sample is connected to u, discard it\n",
    "                    if negative_sample in graph[u] or u in graph[negative_sample] or negative_sample == u:\n",
    "                        continue \n",
    "\n",
    "                    negative_samples_words.append(negative_sample)\n",
    "                    negative_sample = embeddings[negative_sample]\n",
    "                    negative_samples.append(negative_sample)\n",
    "\n",
    "\n",
    "                ## Compute the individual loss\n",
    "                loss = self.loss(embeddings[u], embeddings[v], negative_samples)\n",
    "\n",
    "                ## Compute the partial derivatives of the loss with respect to u, v and the negative examples\n",
    "\n",
    "                # derivative of loss with respect to u\n",
    "                grad_u = -1.0 * self.pdr(embeddings[u], embeddings[v])\n",
    "\n",
    "                # derivative of loss with respect to v\n",
    "                grad_v = -1.0 * self.pdr(embeddings[v], embeddings[u])\n",
    "\n",
    "                # derivative of loss with respect to the negative examples\n",
    "                grad_negatives = []\n",
    "                grad_negatives_temp = self.pdl(embeddings[u], negative_samples)\n",
    "\n",
    "                for (negative_sample, grad_negative) in zip(negative_samples, grad_negatives_temp):\n",
    "                    gradient = grad_negative * self.pdr(negative_sample, embeddings[u])\n",
    "                    grad_negatives.append(gradient)\n",
    "\n",
    "                ## Update embeddings\n",
    "\n",
    "                # update u\n",
    "                embeddings[u] = self.update(embeddings[u], grad_u, self.learning_rate)\n",
    "\n",
    "                # update v\n",
    "                embeddings[v] = self.update(embeddings[v], grad_v, self.learning_rate)\n",
    "\n",
    "                # update negative samples\n",
    "                for (negative_sample, grad_negative, negative_sample_word) in zip(negative_samples, grad_negatives, negative_samples_words):\n",
    "                        embeddings[negative_sample_word] = self.update(negative_sample, grad_negative, self.learning_rate)\n",
    "                                           \n",
    "        test = [embeddings[u] for u in embeddings]\n",
    "        return test\n",
    "        #return embeddings\n",
    "        \n",
    "    def plot_(self, embeddings):\n",
    "        \"\"\"\n",
    "        Function that allows to plot the embedded vectors when the embedding space is 2 dimensional\n",
    "        \"\"\"\n",
    "\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        # plot all the nodes\n",
    "        #for word in embeddings:\n",
    "        #plt.plot(embeddings[word][0], embeddings[word][1], 'bo', label = word)\n",
    "\n",
    "        fig, ax = plt.subplots();\n",
    "\n",
    "        for word in embeddings:\n",
    "            ax.scatter(embeddings[word][0], embeddings[word][1])\n",
    "            #for i, word in enumerate(embeddings):\n",
    "                #ax.annotate(word, (embeddings[word][0], embeddings[word][1]))\n",
    "        plt.show();\n",
    "                \n",
    "\n",
    "    def plot_test(self, test):\n",
    "            \"\"\"\n",
    "            Function that allows to plot the embedded vectors when the embedding space is 2 dimensional\n",
    "            \"\"\"\n",
    "\n",
    "            fig = plt.figure()\n",
    "\n",
    "            # plot all the nodes\n",
    "            for vector in test:\n",
    "                plt.plot(vector[0], vector[1], 'bo')\n",
    "\n",
    "            plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "mammal = wordnet.synset(\"mammal.n.01\")\n",
    "cls = Poincarre_Embeddings(200, 0.1, 5, mammal, 2)\n",
    "embeddings = cls.train()\n",
    "print(embeddings)\n",
    "cls.plot_test(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mammal = wordnet.synset(\"mammal.n.01\")\n",
    "cls = Poincarre_Embeddings(10, 0.01, 3, mammal, 2)\n",
    "graph, embeddings = cls.create_embeddings(2, mammal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Synset('damaraland_mole_rat.n.01')\": [],\n",
       " \"Synset('female_mammal.n.01')\": [],\n",
       " \"Synset('fossorial_mammal.n.01')\": [\"Synset('damaraland_mole_rat.n.01')\",\n",
       "  \"Synset('naked_mole_rat.n.01')\"],\n",
       " \"Synset('mammal.n.01')\": [\"Synset('female_mammal.n.01')\",\n",
       "  \"Synset('fossorial_mammal.n.01')\",\n",
       "  \"Synset('metatherian.n.01')\",\n",
       "  \"Synset('placental.n.01')\",\n",
       "  \"Synset('prototherian.n.01')\",\n",
       "  \"Synset('tusker.n.01')\"],\n",
       " \"Synset('metatherian.n.01')\": [\"Synset('marsupial.n.01')\"],\n",
       " \"Synset('placental.n.01')\": [\"Synset('aardvark.n.01')\",\n",
       "  \"Synset('aquatic_mammal.n.01')\",\n",
       "  \"Synset('bat.n.01')\",\n",
       "  \"Synset('buck.n.05')\",\n",
       "  \"Synset('bull.n.11')\",\n",
       "  \"Synset('carnivore.n.01')\",\n",
       "  \"Synset('cow.n.02')\",\n",
       "  \"Synset('digitigrade_mammal.n.01')\",\n",
       "  \"Synset('doe.n.02')\",\n",
       "  \"Synset('edentate.n.01')\",\n",
       "  \"Synset('fissipedia.n.01')\",\n",
       "  \"Synset('flying_lemur.n.01')\",\n",
       "  \"Synset('hyrax.n.01')\",\n",
       "  \"Synset('insectivore.n.01')\",\n",
       "  \"Synset('lagomorph.n.01')\",\n",
       "  \"Synset('livestock.n.01')\",\n",
       "  \"Synset('pachyderm.n.01')\",\n",
       "  \"Synset('pangolin.n.01')\",\n",
       "  \"Synset('plantigrade_mammal.n.01')\",\n",
       "  \"Synset('primate.n.02')\",\n",
       "  \"Synset('proboscidean.n.01')\",\n",
       "  \"Synset('rodent.n.01')\",\n",
       "  \"Synset('tree_shrew.n.01')\",\n",
       "  \"Synset('unguiculata.n.01')\",\n",
       "  \"Synset('unguiculate.n.01')\",\n",
       "  \"Synset('ungulata.n.01')\",\n",
       "  \"Synset('ungulate.n.01')\",\n",
       "  \"Synset('yearling.n.03')\"],\n",
       " \"Synset('prototherian.n.01')\": [\"Synset('monotreme.n.01')\"],\n",
       " \"Synset('tusker.n.01')\": []}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([\"Synset('prototherian.n.01')\", \"Synset('placental.n.01')\", \"Synset('tusker.n.01')\", \"Synset('metatherian.n.01')\", \"Synset('female_mammal.n.01')\", \"Synset('mammal.n.01')\", \"Synset('damaraland_mole_rat.n.01')\", \"Synset('fossorial_mammal.n.01')\"])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u is:  Synset('prototherian.n.01')\n",
      "v is:  Synset('monotreme.n.01')\n",
      "negative_samples_words : [\"Synset('female_mammal.n.01')\", \"Synset('tusker.n.01')\", \"Synset('metatherian.n.01')\"]\n",
      "negative_samples : [array([-0.00064182,  0.00020491]), array([0.0001629 , 0.00058987]), array([ 0.00081248, -0.00060515])]\n",
      "-----------------------------------------------------\n",
      "u is:  Synset('placental.n.01')\n",
      "v is:  Synset('edentate.n.01')\n",
      "negative_samples_words : [\"Synset('female_mammal.n.01')\", \"Synset('damaraland_mole_rat.n.01')\", \"Synset('fossorial_mammal.n.01')\"]\n",
      "negative_samples : [array([-0.00064182,  0.00020491]), array([-0.00014274, -0.00013274]), array([-0.00064724,  0.00094159])]\n",
      "-----------------------------------------------------\n",
      "u is:  Synset('metatherian.n.01')\n",
      "v is:  Synset('marsupial.n.01')\n",
      "negative_samples_words : [\"Synset('placental.n.01')\", \"Synset('placental.n.01')\", \"Synset('female_mammal.n.01')\"]\n",
      "negative_samples : [array([ 0.00064961, -0.00026617]), array([ 0.00064961, -0.00026617]), array([-0.00064182,  0.00020491])]\n",
      "-----------------------------------------------------\n",
      "u is:  Synset('mammal.n.01')\n",
      "v is:  Synset('fossorial_mammal.n.01')\n",
      "negative_samples_words : [\"Synset('damaraland_mole_rat.n.01')\", \"Synset('damaraland_mole_rat.n.01')\", \"Synset('damaraland_mole_rat.n.01')\"]\n",
      "negative_samples : [array([-0.00014274, -0.00013274]), array([-0.00014274, -0.00013274]), array([-0.00014274, -0.00013274])]\n",
      "-----------------------------------------------------\n",
      "u is:  Synset('fossorial_mammal.n.01')\n",
      "v is:  Synset('naked_mole_rat.n.01')\n",
      "negative_samples_words : [\"Synset('placental.n.01')\", \"Synset('tusker.n.01')\", \"Synset('placental.n.01')\"]\n",
      "negative_samples : [array([ 0.00064961, -0.00026617]), array([0.0001629 , 0.00058987]), array([ 0.00064961, -0.00026617])]\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for u in embeddings:\n",
    "    if len(graph[u]) == 0:\n",
    "        continue\n",
    "    print(\"u is: \", u)\n",
    "                    \n",
    "    # Select v among the hyponyms of u\n",
    "    v = np.random.choice(graph[u])\n",
    "    print(\"v is: \", v)\n",
    "                \n",
    "    # Select negative examples for u\n",
    "    negative_samples, negative_samples_words = [], []  # list of vectors/list of words\n",
    "                \n",
    "    while len(negative_samples) < 3:\n",
    "\n",
    "        # draw sample randomly from data\n",
    "        negative_sample = np.random.choice(list(embeddings.keys()))\n",
    "\n",
    "        # if the drawn sample is connected to u, discard it\n",
    "        if negative_sample in graph[u] or u in graph[negative_sample] or negative_sample == u:\n",
    "            continue \n",
    "\n",
    "        negative_samples_words.append(negative_sample)\n",
    "        negative_sample = embeddings[negative_sample]\n",
    "        negative_samples.append(negative_sample)\n",
    "        \n",
    "    print(\"negative_samples_words : {0}\".format(negative_samples_words))\n",
    "    print(\"negative_samples : {0}\".format(negative_samples))\n",
    "    print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

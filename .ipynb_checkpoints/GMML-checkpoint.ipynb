{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the article on Poincarré Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charlesdognin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Packages import \n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from math import *\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('wordnet')  \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But : \n",
    "- Partir d'un mot wordnet.synset(\"mot\")\n",
    "- Construire un dictionnaire représentant le graphe lié à ce mot, du type {\"mot\" : [liste des hyponymes]}\n",
    "- Construire un dictionnaire du type {\"mot\" : niveau du mot}\n",
    "- Construire un dictionnaire des mots embedded du type {\"mot\" : vecteur}\n",
    "\n",
    "Dans le dictionnaire représentant le graphe, un même mot peut être clé ou valeur.\n",
    "Dans le dictionnaire des mots embedded, chaque mot est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any warm-blooded vertebrate having the skin more or less covered with hair; young are born alive except for the small subclass of monotremes and nourished with milk\n",
      "-------------------------\n",
      "Synset('mammal.n.01')\n"
     ]
    }
   ],
   "source": [
    "# Choose a source word for our graph, here the word \"mammal\", whose level in the graph is 0 (by default)\n",
    "\n",
    "mammal = wordnet.synset(\"mammal.n.01\")\n",
    "print(mammal.definition())  # definition of \"mammal\"\n",
    "print('-------------------------')\n",
    "print(mammal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('female_mammal.n.01'),\n",
       " Synset('fossorial_mammal.n.01'),\n",
       " Synset('metatherian.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('prototherian.n.01'),\n",
       " Synset('tusker.n.01')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyponyms of the source word, i.e. its direct children in the graph\n",
    "mammal.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graphe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-935105592f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmammal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-935105592f7d>\u001b[0m in \u001b[0;36msample_graph\u001b[0;34m(root_node, max_level)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# condition on maximum level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_level\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mgraphe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graphe' is not defined"
     ]
    }
   ],
   "source": [
    "# words network\n",
    "graph = {}\n",
    "\n",
    "# levels of nodes\n",
    "levels = {}\n",
    "\n",
    "# Sample graph as a dictionnary\n",
    "\n",
    "def sample_graph(root_node, max_level=4) :\n",
    "    \"\"\"\n",
    "    Function that samples a hierarchical network from a root node and its hyponyms.\n",
    "    :param root_node: root node of the network\n",
    "    :param max_level: (int) maximum level of the network\n",
    "    :return graph: dictionnary representing the graph {\"node\" : [hyponyms]}\n",
    "    :return levels: dictionnary representing the level of each node {\"node\" : level}\n",
    "    \"\"\"\n",
    "    \n",
    "    # keep track of visited nodes\n",
    "    explored = []\n",
    "    \n",
    "    # keep track of nodes to be checked\n",
    "    queue = [root_node]\n",
    "    \n",
    "    levels = {}\n",
    "    levels[root_node] = 0\n",
    "    \n",
    "    visited = [root_node]\n",
    "    \n",
    "    while queue:\n",
    "        \n",
    "        # take out first node from queue\n",
    "        node = queue.pop(0)\n",
    "        \n",
    "        # condition on maximum level\n",
    "        if levels[node] == max_level :\n",
    "            graphe[node] = []\n",
    "            return\n",
    "        \n",
    "        # mark node as explored node\n",
    "        explored.append(node)\n",
    "        \n",
    "        # sample neighbours of node (i.e. its hyponyms)\n",
    "        neighbours = [neighbour for neighbour in node.hyponyms()]\n",
    "        \n",
    "        # add neighbours to the graph (as children of the node)\n",
    "        graph[node] = neighbours\n",
    "        \n",
    "        # add neighbours of node to queue\n",
    "        for neighbour in neighbours :\n",
    "            if neighbour not in visited :\n",
    "                queue.append(neighbour)\n",
    "                visited.append(neighbour)\n",
    "\n",
    "                levels[neighbour] = levels[node] + 1\n",
    "\n",
    "    print(levels)\n",
    "\n",
    "    return graph, levels\n",
    "\n",
    "# Initialize embedded words\n",
    "\n",
    "def sample_embeddings(graph, dimension):\n",
    "    \"\"\"\n",
    "    Initializes embedded vectors of graph.\n",
    "    :param graph: graph containing words\n",
    "    :param dimension: (int) dimension of the embedding space\n",
    "    :return embeddings: dictionnary of the form {\"node\" : vector}\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = {}\n",
    "    \n",
    "    for node in graph:\n",
    "        embeddings[node] = np.random.uniform(low=-0.001, high=0.001, size=(dimension,))\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Vocabulary\n",
    "graph, levels = sample_graph(mammal) \n",
    "embeddings = sample_embeddings(graph, dimension)\n",
    "data = list(embeddings.keys())\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function when a 2 dimensional embedding space is used\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_graph_2D(embeddings):\n",
    "    \"\"\"\n",
    "    Function that allows to plot the embedded vectors when the embedding space is 2 dimensional\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # plot all the nodes\n",
    "    for word in embeddings:\n",
    "        plt.plot(embeddings[word][0], embeddings[word][1])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Training of the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poincarre_Embeddings:\n",
    "    \n",
    "    def __init__(self, epochs, learning_rate):\n",
    "        self.epochs = 200\n",
    "        self.learning_rate = 0.01\n",
    "        self.number_of_negative_samples = 5\n",
    "        \n",
    "    def distance(self, u, v):\n",
    "        \"\"\"\n",
    "        Computes the distance for the Poincaré disk model between two vectors u and v\n",
    "\n",
    "        Arguments:\n",
    "        u -- first embedded object\n",
    "        v -- second embedded object\n",
    "\n",
    "        Returns:\n",
    "        z -- the ditance between the two objects\n",
    "        \"\"\"\n",
    "\n",
    "        assert norm(u) < 1 and norm(v) < 1\n",
    "        norm2u = norm(u)**2\n",
    "        norm2v = norm(u)**2\n",
    "        norm2distuv = norm(u - v)**2\n",
    "        t = 1 + 2 * (norm2distuv / ((1 - norm2u) * (1 - norm2v)))\n",
    "        z = np.arccosh(t)\n",
    "        return z\n",
    "\n",
    "    def partial_derivative_right(self, theta, x):\n",
    "        \"\"\"\n",
    "        Computes the partial derivative w.r.t theta\n",
    "\n",
    "        Arguments:\n",
    "        theta -- embedding of the object\n",
    "        x -- vector corresponding to the embedding of another object (same dimension as theta)\n",
    "\n",
    "        Returns:\n",
    "        partial -- the derivative (same dimension as theta)  \n",
    "        \"\"\"\n",
    "\n",
    "        alpha = 1.0 - norm(theta)**2\n",
    "        assert len(alpha.shape) == 0\n",
    "\n",
    "        beta = 1.0 - norm(x)**2\n",
    "        assert len(beta.shape) == 0\n",
    "\n",
    "        gamma = 1 + (2 / (alpha * beta)) * norm(alpha - x)**2\n",
    "        assert len(gamma.shape) == 0\n",
    "\n",
    "        partial = 4.0 / (beta * np.sqrt(gamma**2 - 1)) * (((norm(x) - 2 * np.dot(theta, x) + 1) / alpha**2) * theta - (x / alpha))\n",
    "\n",
    "        return partial\n",
    "\n",
    "    def projection(self, theta, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        Projection in the Poincaré disk ball\n",
    "\n",
    "        Parameters:\n",
    "        theta --  embedding of the object\n",
    "        epsilon -- scalar (for stability)\n",
    "\n",
    "        Returns:\n",
    "        theta -- after projection\n",
    "        \"\"\"\n",
    "\n",
    "        if norm(theta) >= 1:\n",
    "            theta = (theta / norm(theta)) - epsilon\n",
    "\n",
    "        return theta\n",
    "\n",
    "\n",
    "    def full_update(self, theta, grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Computes the full update for a single embedding of theta\n",
    "\n",
    "        Parameters:\n",
    "        theta -- current embedding of the object\n",
    "        grad -- gradient of the loss function \n",
    "        learning_rate -- the learning rate \n",
    "\n",
    "        Returns:\n",
    "        theta -- the updated theta\n",
    "        \"\"\"\n",
    "\n",
    "        update = (learning_rate / 4) * (1 - norm(theta)**2)**2 * gradient\n",
    "        theta = projection(theta - update)\n",
    "\n",
    "        return theta\n",
    "    \n",
    "    def loss(self, u, v, negative_samples):\n",
    "        \"\"\"\n",
    "        Computes the loss for a single couple of related nodes (u,v)\n",
    "\n",
    "        Arguments:\n",
    "        u -- embedding of one object\n",
    "        v -- embedding of one object\n",
    "        negative_samples -- set of negative samples for u including v\n",
    "\n",
    "        Returns:\n",
    "        loss -- the value of the loss\n",
    "        \"\"\"\n",
    "        negative_distances = [exp(-dist(u, k)) for k in negative_samples]\n",
    "        loss = -dist(u, v) - log(sum(negative_distances))\n",
    "\n",
    "        return loss \n",
    "    \n",
    "    def partial_derivative_left(self, u, negative_samples, positive=True):\n",
    "        \"\"\"\n",
    "        Computes the partial derivative of the loss w.r.t d(u,v), d(u,v'), where v' is a negative example for u\n",
    "        :param negative_samples: list of negative samples for u\n",
    "        :param positive: if True, then computes the partial derivative of the loss w.r.t d(u,v)\n",
    "        :return: derivative (scalar or list)\n",
    "        \"\"\"\n",
    "        if positive:\n",
    "            derivative = -1\n",
    "\n",
    "        else:\n",
    "            negative_distances = [exp(-dist(u, k)) for k in negative_samples]\n",
    "            derivative = [exp(dist(u, k))/sum(negative_distances) for k in negative_samples]\n",
    "\n",
    "        return derivative \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for epoch in range(number_epochs):\n",
    "\n",
    "            ## Select couple (u, v) and negative samples for u\n",
    "\n",
    "            # Select u\n",
    "            for u in data:\n",
    "\n",
    "                # Select v among the hyponyms of u\n",
    "                v = random.choice(graph[u])\n",
    "\n",
    "                # Select negative examples for u\n",
    "                negative_samples = []  # list of vectors\n",
    "                negative_samples_words = []  # list of words\n",
    "\n",
    "                while len(negative_samples) < number_of_negative_samples :\n",
    "\n",
    "                    # draw sample randomly from data\n",
    "                    negative_sample = random.choice(data)\n",
    "\n",
    "                    # if the drawn sample is connected to u, discard it\n",
    "                    if negative_sample in graph[u] or u in graph[negative_sample] or negative_sample==u:\n",
    "                        continue \n",
    "\n",
    "                    negative_samples_words.append(negative_sample)\n",
    "                    negative_sample = embeddings[negative_sample]\n",
    "                    negative_samples.append(negative_sample)\n",
    "\n",
    "\n",
    "            ## Compute the individual loss\n",
    "            loss = loss(embeddings[u], embeddings[v], negative_samples)\n",
    "\n",
    "\n",
    "\n",
    "            ## Compute the partial derivatives of the loss with respect to u, v and the negative examples\n",
    "\n",
    "            # derivative of loss with respect to u\n",
    "            derivative_u = partial_derivative_left(embeddings[u], negative_samples, positive=True) \\ \n",
    "                * partial_derivative_right(embeddings[u], embeddings[v])\n",
    "\n",
    "            # derivative of loss with respect to v\n",
    "            derivative_v = partial_derivative_left(embeddings[v], negative_samples, positive=True) * \\\n",
    "                partial_derivative_right(embeddings[v], embeddings[u])\n",
    "\n",
    "            # derivative of loss with respect to the negative examples\n",
    "            derivative_negatives = []\n",
    "\n",
    "            derivative_negatives_temp = partial_derivative_left(embeddings[u], negative_samples, positive=False)\n",
    "\n",
    "            for (negative_sample, derivative_negative) in zip(negative_samples, derivative_negatives_temp):\n",
    "                gradient = derivative_negative * partial_derivative_right(negative_sample, embeddings[u])\n",
    "                derivative_negatives.append(gradient)\n",
    "\n",
    "\n",
    "\n",
    "            ## Update embeddings\n",
    "\n",
    "            # update u\n",
    "            embeddings[u] = update(embeddings[u], derivative_u, learning_rate)\n",
    "\n",
    "            # update v\n",
    "            embeddings[v] = update(embeddings[v], derivative_v, learning_rate)\n",
    "\n",
    "            # update negative samples\n",
    "            for (negative_sample, derivative_negative, negative_sample_word) in \\\n",
    "                zip(negative_samples, derivative_negatives, negative_samples_words):\n",
    "                    embeddings[negative_sample_word] = update(negative_sample, derivative_negative, learning_rate)\n",
    "                pass"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

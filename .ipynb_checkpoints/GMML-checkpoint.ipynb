{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the article on Poincarré Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charlesdognin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Packages import \n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from math import *\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('wordnet')  \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But : \n",
    "- Partir d'un mot wordnet.synset(\"mot\")\n",
    "- Construire un dictionnaire représentant le graphe lié à ce mot, du type {\"mot\" : [liste des hyponymes]}\n",
    "- Construire un dictionnaire du type {\"mot\" : niveau du mot}\n",
    "- Construire un dictionnaire des mots embedded du type {\"mot\" : vecteur}\n",
    "\n",
    "Dans le dictionnaire représentant le graphe, un même mot peut être clé ou valeur.\n",
    "Dans le dictionnaire des mots embedded, chaque mot est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any warm-blooded vertebrate having the skin more or less covered with hair; young are born alive except for the small subclass of monotremes and nourished with milk\n",
      "-------------------------\n",
      "Synset('mammal.n.01')\n"
     ]
    }
   ],
   "source": [
    "# Choose a source word for our graph, here the word \"mammal\", whose level in the graph is 0 (by default)\n",
    "\n",
    "mammal = wordnet.synset(\"mammal.n.01\")\n",
    "print(mammal.definition())  # definition of \"mammal\"\n",
    "print('-------------------------')\n",
    "print(mammal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('female_mammal.n.01'),\n",
       " Synset('fossorial_mammal.n.01'),\n",
       " Synset('metatherian.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('prototherian.n.01'),\n",
       " Synset('tusker.n.01')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyponyms of the source word, i.e. its direct children in the graph\n",
    "mammal.hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Training of the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poincarre_Embeddings:\n",
    "    \n",
    "    def __init__(self, epochs, learning_rate, nb_negs, root_node, dimension):\n",
    "        \"\"\"\n",
    "        Object providing the embedding for words related by hypermnemy relations using \n",
    "        hyperbolic geometry.\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "        epochs -- number of epochs/iterations\n",
    "        learning_rate -- the learning rate for update of the embedding\n",
    "        nb_negs -- number of negative samples\n",
    "        root_node -- the higher word in the hierarchy, the format must be: wordnet.synset(\"word.n.01\")\n",
    "        dimension -- the embedding dimension\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nb_negs = nb_negs\n",
    "        self.root_node = root_node\n",
    "        self.dimension = dimension\n",
    "        \n",
    "    # Sample graph as a dictionnary\n",
    "\n",
    "    def sample_graph(self, root_node, max_level = 7) :\n",
    "        \"\"\"\n",
    "        Function that samples a hierarchical network from a root node and its hyponyms.\n",
    "        :param root_node: root node of the network\n",
    "        :param max_level: (int) maximum level of the network\n",
    "        :return graph: dictionnary representing the graph {\"node\" : [hyponyms]}\n",
    "        :return levels: dictionnary representing the level of each node {\"node\" : level}\n",
    "        \"\"\"\n",
    "\n",
    "        graph = {}\n",
    "\n",
    "        # keep track of visited nodes\n",
    "        explored = []\n",
    "\n",
    "        # keep track of nodes to be checked\n",
    "        queue = [root_node]\n",
    "\n",
    "        levels = {}\n",
    "        levels[str(root_node)] = 0\n",
    "\n",
    "        visited = [str(root_node)]\n",
    "\n",
    "        while queue:\n",
    "\n",
    "            # take out first node from queue\n",
    "            node = queue.pop(0)  # node n'est PAS un str\n",
    "\n",
    "            # condition on maximum level\n",
    "            if levels[str(node)] == max_level:\n",
    "                graph[str(node)] = []\n",
    "                break;\n",
    "\n",
    "            # mark node as explored node\n",
    "            explored.append(str(node))  # explored est un str\n",
    "\n",
    "            # sample neighbours of node (i.e. its hyponyms)\n",
    "            neighbours = [neighbour for neighbour in node.hyponyms()]  # ce sont pas des str\n",
    "            neighbours_str = [str(neighbour) for neighbour in node.hyponyms()]\n",
    "\n",
    "            # add neighbours to the graph (as children of the node)\n",
    "            graph[str(node)] = neighbours_str\n",
    "\n",
    "            # add neighbours of node to queue\n",
    "            for neighbour in neighbours :   # no str\n",
    "                if str(neighbour) not in visited :\n",
    "                    queue.append(neighbour) # no str\n",
    "                    visited.append(str(neighbour))\n",
    "                    levels[str(neighbour)] = levels[str(node)] + 1\n",
    "\n",
    "        return graph, levels\n",
    "    \n",
    "    def sample_embeddings(self, graph, dimension):\n",
    "        \"\"\"\n",
    "        Initializes embedded vectors of graph.\n",
    "        :param graph: graph containing words\n",
    "        :param dimension: (int) dimension of the embedding space\n",
    "        :return embeddings: dictionnary of the form {\"node\" : vector}\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings = {}\n",
    "\n",
    "        for word in graph:\n",
    "            embeddings[word] = np.random.uniform(low=-0.001, high=0.001, size=(dimension,))\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    def create_embeddings(self, dimension, root_node):\n",
    "        \"\"\"\n",
    "        Creates embeddings for words\n",
    "        \"\"\"\n",
    "        graph, levels = self.sample_graph(self.root_node) \n",
    "        embeddings = sample_embeddings(graph, self.dimension)\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "\n",
    "    def dist(self, u, v):\n",
    "        \"\"\"\n",
    "        Computes the distance for the Poincaré disk model between two vectors u and v\n",
    "\n",
    "        Arguments:\n",
    "        u -- first embedded object\n",
    "        v -- second embedded object\n",
    "\n",
    "        Returns:\n",
    "        z -- the ditance between the two objects\n",
    "        \"\"\"\n",
    "\n",
    "        assert norm(u) < 1 and norm(v) < 1\n",
    "        norm2u = norm(u)**2\n",
    "        norm2v = norm(u)**2\n",
    "        norm2distuv = norm(u - v)**2\n",
    "        t = 1 + 2 * (norm2distuv / ((1 - norm2u) * (1 - norm2v)))\n",
    "        z = np.arccosh(t)\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def pdr(self, theta, x):\n",
    "        \"\"\"\n",
    "        Computes the partial derivative w.r.t theta\n",
    "\n",
    "        Arguments:\n",
    "        theta -- embedding of the object\n",
    "        x -- vector corresponding to the embedding of another object (same dimension as theta)\n",
    "\n",
    "        Returns:\n",
    "        partial -- the derivative (same dimension as theta)  \n",
    "        \"\"\"\n",
    "\n",
    "        alpha = 1.0 - norm(theta)**2\n",
    "        assert len(alpha.shape) == 0\n",
    "        beta = 1.0 - norm(x)**2\n",
    "        assert len(beta.shape) == 0\n",
    "        gamma = 1 + (2 / (alpha * beta)) * norm(alpha - x)**2\n",
    "        assert len(gamma.shape) == 0\n",
    "        partial = 4.0 / (beta * np.sqrt(gamma**2 - 1)) * (((norm(x) - 2 * np.dot(theta, x) + 1) / alpha**2) * theta - (x / alpha))\n",
    "\n",
    "        return partial\n",
    "\n",
    "    def proj(self, theta, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        Projection in the Poincaré disk ball\n",
    "\n",
    "        Parameters:\n",
    "        theta --  embedding of the object\n",
    "        epsilon -- scalar (for stability)\n",
    "\n",
    "        Returns:\n",
    "        theta -- after projection\n",
    "        \"\"\"\n",
    "\n",
    "        if norm(theta) >= 1:\n",
    "            theta = (theta / norm(theta)) - epsilon\n",
    "\n",
    "        return theta\n",
    "\n",
    "    def update(self, theta, grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Computes the full update for a single embedding of theta\n",
    "\n",
    "        Parameters:\n",
    "        theta -- current embedding of the object\n",
    "        grad -- gradient of the loss function \n",
    "        learning_rate -- the learning rate \n",
    "\n",
    "        Returns:\n",
    "        theta -- the updated theta\n",
    "        \"\"\"\n",
    "\n",
    "        upd = (learning_rate / 4) * (1 - norm(theta)**2)**2 * grad\n",
    "        theta = self.proj(theta - upd)\n",
    "        assert theta.shape == upd.shape\n",
    "\n",
    "        return theta\n",
    "    \n",
    "    def loss(self, u, v, negative_samples):\n",
    "        \"\"\"\n",
    "        Computes the loss for a single couple of related nodes (u,v)\n",
    "\n",
    "        Arguments:\n",
    "        u -- embedding of one object\n",
    "        v -- embedding of one object\n",
    "        negative_samples -- set of negative samples for u including v\n",
    "\n",
    "        Returns:\n",
    "        loss -- the value of the loss\n",
    "        \"\"\"\n",
    "        negative_distances = [np.exp(-self.dist(u, k)) for k in negative_samples]\n",
    "        loss = -self.dist(u, v) - np.log(np.sum(negative_distances))\n",
    "\n",
    "        return loss \n",
    "    \n",
    "    def pdl(self, u, negative_samples):\n",
    "        \"\"\"\n",
    "        Computes the partial derivative of the loss w.r.t d(u,v), d(u,v'), where v' is a negative example for u\n",
    "        \n",
    "        Arguments:\n",
    "        u -- embedding of one object\n",
    "        negative_samples -- list of negative samples for u\n",
    "        positive -- boolean, computes the partial derivative of the loss w.r.t d(u,v) if True\n",
    "        \n",
    "        Returns:\n",
    "        derivative -- the partial derivative (scalar or list)\n",
    "        \"\"\"\n",
    "    \n",
    "        negative_distances = [np.exp(-dist(u, k)) for k in negative_samples]\n",
    "        derivative = [np.exp(dist(u, k)) / np.sum(negative_distances) for k in negative_samples]\n",
    "\n",
    "        return derivative \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        embeddings = self.create_embeddings(self.dimension, self.root_node)\n",
    "        print(\"embeddings is: \", embeddings)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            ## Select couple (u, v) and negative samples for u\n",
    "\n",
    "            # Select word\n",
    "            for u in embeddings.keys():\n",
    "                print(\"u is: \", u)\n",
    "                if len(graph[u]) == 0:\n",
    "                    continue;\n",
    "                    \n",
    "                # Select v among the hyponyms of u\n",
    "                v = np.random.choice(graph[u])\n",
    "                \n",
    "                # Select negative examples for u\n",
    "                negative_samples, negative_samples_words = [], []  # list of vectors/list of words\n",
    "                \n",
    "                while len(negative_samples) < self.nb_negs:\n",
    "\n",
    "                    # draw sample randomly from data\n",
    "                    negative_sample = np.random.choice(list(embeddings.keys()))\n",
    "\n",
    "                    # if the drawn sample is connected to u, discard it\n",
    "                    if negative_sample in graph[u] or u in graph[negative_sample] or negative_sample == u:\n",
    "                        continue \n",
    "\n",
    "                    negative_samples_words.append(negative_sample)\n",
    "                    negative_sample = embeddings[negative_sample]\n",
    "                    negative_samples.append(negative_sample)\n",
    "\n",
    "\n",
    "            ## Compute the individual loss\n",
    "            loss = self.loss(embeddings[u], embeddings[v], negative_samples)\n",
    "\n",
    "            ## Compute the partial derivatives of the loss with respect to u, v and the negative examples\n",
    "\n",
    "            # derivative of loss with respect to u\n",
    "            grad_u = -1.0 * self.pdr(embeddings[u], embeddings[v])\n",
    "\n",
    "            # derivative of loss with respect to v\n",
    "            grad_v = -1.0 * self.pdr(embeddings[v], embeddings[u])\n",
    "\n",
    "            # derivative of loss with respect to the negative examples\n",
    "            grad_negatives = []\n",
    "            grad_negatives_temp = self.pdl(embeddings[u], negative_samples)\n",
    "\n",
    "            for (negative_sample, grad_negative) in zip(negative_samples, grad_negatives_temp):\n",
    "                gradient = grad_negative * self.pdr(negative_sample, embeddings[u])\n",
    "                grad_negatives.append(gradient)\n",
    "\n",
    "            ## Update embeddings\n",
    "\n",
    "            # update u\n",
    "            embeddings[u] = self.update(embeddings[u], grad_u, self.learning_rate)\n",
    "\n",
    "            # update v\n",
    "            embeddings[v] = self.update(embeddings[v], grad_v, self.learning_rate)\n",
    "\n",
    "            # update negative samples\n",
    "            for (negative_sample, grad_negative, negative_sample_word) in zip(negative_samples, grad_negatives, negative_samples_words):\n",
    "                    embeddings[negative_sample_word] = update(negative_sample, grad_negative, self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Synset('farm_horse.n.01')\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-37efc827a367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmammal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mammal.n.01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoincarre_Embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmammal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-0ba968e0a3f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;31m## Compute the individual loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;31m## Compute the partial derivatives of the loss with respect to u, v and the negative examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Synset('farm_horse.n.01')\""
     ]
    }
   ],
   "source": [
    "mammal = wordnet.synset(\"mammal.n.01\")\n",
    "cls = Poincarre_Embeddings(10, 0.01, 3, mammal, 2)\n",
    "cls.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a16477278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEjxJREFUeJzt3X2MnWWZx/Hv1U6HnkgRY8dNZSjt\nalvs7uqiE2JiWVCQtmQDW7tL2sS69a3uuoPRtI2AhELJ4gY3SkxZpTagNRG223RrayCNixipCwtD\nkCpDagZcylCSVnQIxNG+7LV/nANOp1PmzMyZt97fTzLpee7nPs+5rs7Mb565zznzRGYiSTr9TRnv\nAiRJY8PAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBWiabweeObMmTlnzpzxenhJ\nmpQef/zxX2dmy3DuO26BP2fOHDo6Osbr4SVpUoqI54Z7X5d0JKkQBr4kFcLAl6RCGPiSVAgDX5IK\nYeBLUiEMfEmaoG6//XbuvPPOAfdFxNKIeCUiMiJ213M8A1+SJqidO3eye/cps/w5oB34ab3HM/Al\naZTs3buXM844gwULFjB9+nTmzJnDbbfdxllnnUVzczN33303AM888wyzZs2iUqlw5plnsn37dvbu\n3ctDDz3E/fffT6VSYdOmTSccOzM7M/M7wLF66zHwJWkUHTlyhFtvvZVXX32VQ4cOsWXLFnp6eli3\nbh3XX389AMuWLeP888+nt7eXm2++mVWrVrFo0SIuuugili5dSm9vL+3t7SOuZdDAj4i7IuJQRPzi\nFPsjIr4eEV0RsS8i3jviqiTpNDFt2jSWL19OU1MTs2bN4rLLLmPKlClcfvnl9PT0ANDV1cWGDRsA\nWLt2LUePHuXAgQMNr6WeM/xvA0veYP9SYF7tYw3wjZGXJUmT184nXuAD//Ij/vYb/82x/6tuA0QE\nlUoFgKamJjIT4PV/+5oypfELMIMeMTN/AvzmDaZcBWzNqkeAsyNiVqMKlKTJZOcTL3Ddjp/zQk8v\nAAlct+Pnr4f+QObPn88tt9wCVF+Z09zcTGtrKzNmzOCVV15pWG2N+BFyDvB8n+3u2pgkFecre/bT\ne/T4CWO9R4/zlT37T3mfHTt20NnZSaVS4YYbbmDr1q0AtLe38+ijjw74pG1E/EVEHAMuBv46Io5F\nxBtmbyP+PHIMMHby7yfVAtdQXfZh9uzZDXhoSZpYDtbO7AGmn/tnnLfuP18f/1VX1+v7Fi1axO9/\n/3sA3vGOd/Diiy+edKzFixfT29t70jhAZv6cIWZ4I87wu4Fz+2y3AgcHmpiZmzOzLTPbWlqG9ff7\nJWlCe/vZlSGNj6VGBP4u4GO1V+u8H3g5M0/+USVJBVi/eAGVaVNPGKtMm8r6xQvGqaI/GvTXgYi4\nB7gEmBkR3cAGYBpAZn4TuA+4AugCfgd8fLSKlaSJ7m8uqC6jf2XPfg729PL2syusX7zg9fHxFAO9\nHGgstLW1pZc4lKShiYjHM7NtOPf1nbaSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJek\nQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqE\ngS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYWoK/AjYklE7I+Iroi4doD9\n50XEAxGxLyJ+HBGtjS9VkjQSgwZ+REwF7gCWAguBlRGxsN+0fwW2Zua7gY3AlxtdqCRpZOo5w78Q\n6MrMZzPzCHAvcFW/OQuBB2q3HxxgvyRpnNUT+OcAz/fZ7q6N9fUksLx2exkwIyLeOvLyJEmNUk/g\nxwBj2W97HXBxRDwBXAy8ABw76UARayKiIyI6Dh8+PORiJUnDV0/gdwPn9tluBQ72nZCZBzPzI5l5\nAfCl2tjL/Q+UmZszsy0z21paWkZQtiRpqOoJ/MeAeRExNyKagRXArr4TImJmRLx2rOuAuxpbpiRp\npAYN/Mw8BrQDe4CngW2Z+VREbIyIK2vTLgH2R8QvgT8B/nmU6pUkDVNk9l+OHxttbW3Z0dExLo8t\nSZNVRDyemW3Dua/vtJWkQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJU\nCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw\n8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFqCvwI2JJROyPiK6IuHaA/bMj4sGI\neCIi9kXEFY0vVZI0EoMGfkRMBe4AlgILgZURsbDftBuAbZl5AbAC+LdGFypJGpl6zvAvBLoy89nM\nPALcC1zVb04CZ9Vuvxk42LgSJUmNUE/gnwM832e7uzbW103ARyOiG7gPuGagA0XEmojoiIiOw4cP\nD6NcSdJw1RP4McBY9tteCXw7M1uBK4DvRsRJx87MzZnZlpltLS0tQ69WkjRs9QR+N3Bun+1WTl6y\n+SSwDSAzHwamAzMbUaAkqTHqCfzHgHkRMTcimqk+Kbur35wDwKUAEfEuqoHvmo0kTSCDBn5mHgPa\ngT3A01RfjfNURGyMiCtr09YCn46IJ4F7gNWZ2X/ZR5I0jprqmZSZ91F9Mrbv2I19bncCH2hsaZKk\nRvKdtpJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkq\nhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY\n+JJUCANfkgph4EtSIQx8SSqEgS9Jhagr8CNiSUTsj4iuiLh2gP1fi4if1T5+GRE9jS9VkjQSTYNN\niIipwB3Ah4Fu4LGI2JWZna/Nycwv9Jl/DXDBKNQqSRqBes7wLwS6MvPZzDwC3Atc9QbzVwL3NKI4\nSVLj1BP45wDP99nuro2dJCLOA+YCPxp5aZKkRqon8GOAsTzF3BXA9sw8PuCBItZEREdEdBw+fLje\nGiVJDVBP4HcD5/bZbgUOnmLuCt5gOSczN2dmW2a2tbS01F+lJGnE6gn8x4B5ETE3Ipqphvqu/pMi\nYgHwFuDhxpYoSWqEQQM/M48B7cAe4GlgW2Y+FREbI+LKPlNXAvdm5qmWeyRJ42jQl2UCZOZ9wH39\nxm7st31T48qSJDWa77SVpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiS\nVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mF\nMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9Jhagr8CNiSUTsj4iuiLj2FHOujojO\niHgqIr7X2DIlSSPVNNiEiJgK3AF8GOgGHouIXZnZ2WfOPOA64AOZ+duIeNtoFSxJGp56zvAvBLoy\n89nMPALcC1zVb86ngTsy87cAmXmosWVKkkaqnsA/B3i+z3Z3bayv+cD8iPhpRDwSEUsGOlBErImI\njojoOHz48PAqliQNSz2BHwOMZb/tJmAecAmwEtgSEWefdKfMzZnZlpltLS0tQ61VkjQC9QR+N3Bu\nn+1W4OAAc76fmUcz81fAfqo/ACRJE0Q9gf8YMC8i5kZEM7AC2NVvzk7ggwARMZPqEs+zjSxUkjQy\ngwZ+Zh4D2oE9wNPAtsx8KiI2RsSVtWl7gJciohN4EFifmS+NVtGSpKGLzP7L8WOjra0tOzo6xuWx\nJWmyiojHM7NtOPf1nbaSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+S\nCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQ\nBr4kFcLAl6RCGPiSVIgiA//222/nzjvvHHDfZz/7WSqVCpVKhRkzZrBt27Yxrk6SRkeRgb9z5052\n79494L73vOc9dHZ20tvby/r16/nEJz4xxtVJ0uiYlIG/d+9ezjjjDBYsWMD06dOZM2cOt912G2ed\ndRbNzc3cfffdADzzzDPMmjWLSqXCmWeeyfbt29m7dy8PPfQQ999/P5VKhU2bNp1w7M985jPMnTsX\ngFWrVtHb2zvm/UnSaJiUgQ9w5MgRbr31Vl599VUOHTrEli1b6OnpYd26dVx//fUALFu2jPPPP5/e\n3l5uvvlmVq1axaJFi7joootYunQpvb29tLe3n/IxrrnmGt75zneOVUuSNKrqCvyIWBIR+yOiKyKu\nHWD/6og4HBE/q318qvGlnmjatGksX76cpqYmZs2axWWXXcaUKVO4/PLL6enpAaCrq4sNGzYAsHbt\nWo4ePcqBAwfqOv5Xv/pVfvjDH/KDH/xg1HqQpLE0aOBHxFTgDmApsBBYGRELB5j675n5l7WPLQ2u\ns2rfNvjan8NdS5iSx6rb1RqpVCoANDU1kZkAr//b15Qpg/+M2759O1/84hfZtWsX8+bNa2ADkjR+\n6jnDvxDoysxnM/MIcC9w1eiWNYB922D35+Dl54GEzOr2vlO/imb+/PnccsstQPWVOc3NzbS2tjJj\nxgxeeeWVAe/z8MMPs3LlSjZt2sTixYtHoxNJGhf1BP45wPN9trtrY/0tj4h9EbE9Is5tSHV9PbAR\njvZ7AvVob3X8FHbs2EFnZyeVSoUbbriBrVu3AtDe3s6jjz464JO2q1ev5vjx43z+85+nUqnwpje9\nqeGtSNJ4iIGWPU6YEPF3wOLM/FRtexVwYWZe02fOW4FXM/MPEfEPwNWZ+aEBjrUGWAMwe/bs9z33\n3HP1V3rT2cBAtQbc1FP/cSRpEouIxzOzbTj3recMvxvoe8beChzsOyEzX8rMP9Q2vwW8b6ADZebm\nzGzLzLaWlpahVfrm1qGNS5JOUE/gPwbMi4i5EdEMrAB29Z0QEbP6bF4JPN24EmsuvRGmVU4cm1ap\njkuSBtU02ITMPBYR7cAeYCpwV2Y+FREbgY7M3AV8LiKuBI4BvwFWN7zSd19d/feBjfByd/XM/tIb\n/zguSXpDg67hj5a2trbs6OgYl8eWpMlqtNfwJUmnAQNfkgph4EtSIQx8SSqEgS9JhRi3V+lExGFg\nCG+1PcFM4NcNLGeis9/Tm/2evkaj1/Myc4jvXK0at8AfiYjoGO7LkiYj+z292e/pa6L16pKOJBXC\nwJekQkzWwN883gWMMfs9vdnv6WtC9Top1/AlSUM3Wc/wJUlDNKEDfyJePH00DdZvbc7VEdEZEU9F\nxPfGusZGquPz+7U+n9tfRsSkvdJNHb3OjogHI+KJ2pXjrhiPOhuljn7Pi4gHar3+OCIm7YUtIuKu\niDgUEb84xf6IiK/X/i/2RcR7x7rG12XmhPyg+qeYnwH+FGgGngQW9puzGtg03rWOYb/zgCeAt9S2\n3zbedY9mv/3mX0P1T3OPe+2j9LndDPxj7fZC4H/Hu+5R7vc/gL+v3f4Q8N3xrnsE/f4V8F7gF6fY\nfwVwPxDA+4H/Ga9aJ/IZ/sS4ePrYqaffTwN3ZOZvATLz0BjX2EhD/fyuBO4Zk8oar55eEzirdvvN\n9Luq3CRTT78LgQdqtx8cYP+kkZk/oXodkFO5CtiaVY8AZ/e7aNSYmciBPzEunj526ul3PjA/In4a\nEY9ExJIxq67x6v38EhHnAXOBH41BXaOhnl5vAj4aEd3AfVR/o5ms6un3SWB57fYyYEbt2tino7q/\n1kfbRA78GGCs/0uKdgNzMvPdwH8B3xn1qkZPPf02UV3WuYTqGe+WiDh7lOsaLfX0+5oVwPbMPD6K\n9YymenpdCXw7M1upLgF8NyIm8vfnG6mn33XAxRHxBHAx8ALVK+adjobytT6qJvIXVMMunj5JDNpv\nbc73M/NoZv4K2E/1B8BkVE+/r1nB5F3Ogfp6/SSwDSAzHwamU/07LJNRPd+7BzPzI5l5AfCl2tjL\nY1fimBrK1/qomsiBPzEunj52Bu0X2Al8ECAiZlJd4nl2TKtsnHr6JSIWAG8BHh7j+hqpnl4PAJcC\nRMS7qAb+4TGtsnHq+d6d2ec3mOuAu8a4xrG0C/hY7dU67wdezswXx6OQQS9iPl5yolw8fYzU2e8e\n4PKI6ASOA+sz86Xxq3r46uwXqksd92bt5Q6TUZ29rgW+FRFfoPrr/urJ2nOd/V4CfDkiEvgJ8E/j\nVvAIRcQ9VPuZWXsOZgMwDSAzv0n1OZkrgC7gd8DHx6dS32krScWYyEs6kqQGMvAlqRAGviQVwsCX\npEIY+JJUCANfkgph4EtSIQx8SSrE/wP+bA1T7OdPgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a16fa3e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting function when a 2 dimensional embedding space is used\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_graph_2D(embeddings):\n",
    "    \"\"\"\n",
    "    Function that allows to plot the embedded vectors when the embedding space is 2 dimensional\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # plot all the nodes\n",
    "    #for word in embeddings:\n",
    "        #plt.plot(embeddings[word][0], embeddings[word][1], 'bo', label = word)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for word in embeddings:\n",
    "        ax.scatter(embeddings[word][0], embeddings[word][1])\n",
    "        for i, word in enumerate(embeddings):\n",
    "            ax.annotate(word, (embeddings[word][0], embeddings[word][1]))\n",
    "    plt.show()\n",
    "    \n",
    "x = np.array([1, 1])\n",
    "y = np.array([0.5, 0.5])\n",
    "test = {\"mot 1\": x, \"mot 2\": y}\n",
    "\n",
    "plot_graph_2D(test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
